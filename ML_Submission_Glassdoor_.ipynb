{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "89xtkJwZ18nB",
        "GMQiZwjn3iu7",
        "-oLEiFgy-5Pf",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahildubey08/ML-Submission-Glassdoor/blob/main/ML_Submission_Glassdoor_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Glassdoor Jobs Salary Prediction (Machine Learning)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is aim to develop a machine learning model to predit job salaries such as job title, company, location, required skills, experience level, and other relevent attribute to estimate salary ranges. By leveraging regression algorithms comprehensive data preprocessing, the model provides accurate salary predictions that can benefit both job seekers and employers in making informed decisions about compensation expectations and market trends."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "In today's competitive job market, both job seekers and employers face significant challenges in determining appropriate salary ranges. Job applicants often struggle to understand their market worth, while employers find it difficult to set competitive compensation packages that attract top talent without overpaying."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/glassdoor_jobs.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7258ab30"
      },
      "source": [
        "display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "n1m_QxD3eJvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that this data contains total 956 rows and 15 columns and has no duplicate and null values."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "'Job Title', 'Salary Estimate', 'Job Description', 'Rating', 'Company Name', 'Location', 'Headquarters', 'Size', 'Founded', 'Type of ownership', 'Industry', 'Sector', 'Revenue', 'Competitors'"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "\n",
        "**Job Title:** The title of the job posting (e.g., \"Data Scientist\", \"Healthcare Data Scientist\").\n",
        "\n",
        "**Salary Estimate:** The estimated salary range for the job, provided as a string (e.g., \"$53K-$91K (Glassdoor est.)\"). Some entries have a value of -1, which likely indicates missing data.\n",
        "\n",
        "**Job Description:** A detailed description of the job, including responsibilities, requirements, qualifications, and sometimes benefits. This is a string of text.\n",
        "\n",
        "**Rating:** The company's overall rating on Glassdoor (on a scale of 1 to 5). This is a numeric field.\n",
        "\n",
        "**Company Name:** The name of the company, and sometimes the company's rating is appended (e.g., \"Tecolote Research 3.8\").\n",
        "\n",
        "**Location:** The city and state where the job is located (e.g., \"Albuquerque, NM\").\n",
        "\n",
        "**Headquarters:** The city and state where the company's headquarters is located (e.g., \"Goleta, CA\").\n",
        "\n",
        "**Size:** The size of the company in terms of number of employees, given as a range (e.g., \"501 to 1000 employees\").\n",
        "\n",
        "**Founded:** The year the company was founded.\n",
        "\n",
        "**Type of ownership:** The type of company ownership (e.g., \"Company - Private\", \"Company - Public\", \"Government\", etc.).\n",
        "\n",
        "**Industry:** The industry in which the company operates (e.g., \"Aerospace & Defense\").\n",
        "\n",
        "**Sector:** The sector of the company (e.g., \"Aerospace & Defense\").\n",
        "\n",
        "**Revenue:** The revenue of the company, given as a range (e.g., \"$50 to $100 million (USD)\"). Some entries are \"Unknown / Non-Applicable\".\n",
        "\n",
        "**Competitors:** The competitors of the company. Many entries have -1, which likely indicates no data or no competitors listed."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Create a new DataFrame df_clean\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Handle salary estimate including 'Employer Provided Salary' and other formats\n",
        "df_clean['Salary Estimate'] = df_clean['Salary Estimate'].apply(lambda x: x.replace('(Glassdoor est.)', '').replace('(Employer est.)', '').strip())\n",
        "\n",
        "# Extract min and max salary, handling different formats\n",
        "def parse_salary(salary_str):\n",
        "    try:\n",
        "        if 'Employer Provided Salary:' in salary_str:\n",
        "            salary_str = salary_str.replace('Employer Provided Salary:', '').strip()\n",
        "            # Handle cases like '150K' or '150'\n",
        "            if 'K' in salary_str:\n",
        "                 min_sal = int(salary_str.replace('$', '').replace('K', '')) * 1000\n",
        "            else:\n",
        "                 min_sal = int(salary_str.replace('$', '')) * 1000\n",
        "            max_sal = min_sal # Assuming the employer provided salary is a single value\n",
        "        else:\n",
        "            sal_parts = salary_str.split('-')\n",
        "            if len(sal_parts) == 2:\n",
        "                min_sal = int(sal_parts[0].replace('$', '').replace('K', '')) * 1000\n",
        "                max_sal = int(sal_parts[1].replace('$', '').replace('K', '')) * 1000\n",
        "            else:\n",
        "                min_sal = np.nan\n",
        "                max_sal = np.nan\n",
        "    except ValueError:\n",
        "        min_sal = np.nan\n",
        "        max_sal = np.nan\n",
        "    return min_sal, max_sal\n",
        "\n",
        "df_clean[['min_salary', 'max_salary']] = df_clean['Salary Estimate'].apply(lambda x: pd.Series(parse_salary(x)))\n",
        "\n",
        "# Calculate average salary\n",
        "df_clean['avg_salary'] = (df_clean['min_salary'] + df_clean['max_salary']) / 2\n",
        "\n",
        "# Replace -1 with NaN in the new salary columns\n",
        "df_clean[['min_salary', 'max_salary', 'avg_salary']] = df_clean[['min_salary', 'max_salary', 'avg_salary']].replace(-1, np.nan)\n",
        "\n",
        "# Categorize job titles\n",
        "def categorize_job_title(title):\n",
        "    title = title.lower()\n",
        "    if 'data scientist' in title:\n",
        "        return 'Data Scientist'\n",
        "    elif 'data engineer' in title:\n",
        "        return 'Data Engineer'\n",
        "    elif 'analyst' in title:\n",
        "        return 'Analyst'\n",
        "    elif 'manager' in title:\n",
        "        return 'Manager'\n",
        "    elif 'director' in title:\n",
        "        return 'Manager'\n",
        "    elif 'research scientist' in title:\n",
        "        return 'Data Scientist'\n",
        "    elif 'machine learning' in title:\n",
        "        return 'Data Scientist'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df_clean['Job Role'] = df_clean['Job Title'].apply(categorize_job_title)\n",
        "\n",
        "# Display the head of the cleaned DataFrame\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db63d6bb"
      },
      "source": [
        "# List of columns to drop - Adjusted to keep 'Job Description' for later text processing\n",
        "columns_to_drop = [\n",
        "    'Unnamed: 0', 'Salary Estimate', 'Job Title',\n",
        "    'Company Name', 'Location', 'Headquarters', 'Size', 'Revenue', 'Competitors',\n",
        "    # 'Job Description' and related text processing columns are kept until after text vectorization\n",
        "    # 'description_tokens', 'description_lemmatized', 'description_pos_tags',\n",
        "    # 'description_lemmatized_str', 'simple_title', 'job_state', 'Full_State'\n",
        "]\n",
        "\n",
        "# Exclude 'Job Description' from the list of columns to be dropped if it's there\n",
        "# This ensures it remains for text processing later.\n",
        "actual_columns_to_drop = [col for col in columns_to_drop if col in df_clean.columns and col != 'Job Description']\n",
        "\n",
        "df_clean.drop(columns=actual_columns_to_drop, inplace=True)\n",
        "\n",
        "print(f\"Columns remaining: {df_clean.columns.tolist()}\")\n",
        "display(df_clean.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2362d060"
      },
      "source": [
        "# Apply tokenization to the 'Job Description' column\n",
        "df_clean['description_tokens'] = df_clean['Job Description'].apply(tokenize_text)\n",
        "\n",
        "# Display the head of the DataFrame with the new column\n",
        "display(df_clean.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "By doing the data wrangling.\n",
        "\n",
        "\n",
        "1-I have Removes explanatory text from salary estimates to extract pure salary ranges\n",
        "\n",
        "2-Then I have created the minimum, maximum and average salary range column.\n",
        "\n",
        "3-Applies the parse_salary function to each row in 'Salary Estimate.\n",
        "\n",
        "4-For safety measures I have replace -1 value with Nan\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Salary Distribution Salary Distribution by Job Role (Box Plot)\n",
        "\n"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=df_clean[df_clean['Job Role'] != 'Other'],\n",
        "            x='Job Role', y='avg_salary')\n",
        "plt.title('Salary Distribution by Tech Job Role', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Average Salary ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I choose this boxplot chart for visualizing the salary distribution by job role because it is an effective way to show the distribution of a numerical variable (average salary) across different categories (job roles). A box plot clearly displays the median, quartiles, and potential outliers for each job roles, making it easy to compare the salary ranges and central tendencies of different roles at a glance.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "Based on the box plot visualizing salary distribution by tech job role:\n",
        "\n",
        "*   **Data Scientist and Data Engineer** roles appear to have higher median salaries and a wider salary range compared to 'Analyst' roles.\n",
        "*   **Analyst** roles generally show a lower median salary and a tighter salary distribution.\n",
        "*   There are **outliers** in several job roles, indicating some positions within those categories have significantly higher or lower salaries than the typical range.\n",
        "*   The **Manager** role also shows a relatively high median salary, which is expected due to the nature of the role."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n"
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Impact of Company Size on Salaries"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Temporarily re-add 'Size' column to df_clean from the original df for categorization\n",
        "# This assumes the index aligns or needs to be handled if not.\n",
        "# Given that df_clean was a copy of df and subsequent operations kept the index, this should be fine.\n",
        "if 'Size' not in df_clean.columns and 'Size' in df.columns:\n",
        "    df_clean['Size'] = df['Size']\n",
        "\n",
        "# Categorize company size\n",
        "def categorize_company_size(size):\n",
        "    if 'to' in str(size): # Convert to string to handle potential non-string values like -1\n",
        "        return str(size).replace(' employees', '').strip()\n",
        "    elif '+' in str(size):\n",
        "        return str(size).replace(' employees', '').strip()\n",
        "    elif str(size) == '-1':\n",
        "        return 'Unknown'\n",
        "    else:\n",
        "        return str(size) # Ensure output is string\n",
        "\n",
        "df_clean['Company Size Category'] = df_clean['Size'].apply(categorize_company_size)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "company_size_order = ['1 to 50', '51 to 200', '201 to 500',\n",
        "                     '501 to 1000', '1001 to 5000', '5001 to 10000', '10000+', 'Unknown']\n",
        "sns.barplot(data=df_clean, x='Company Size Category', y='avg_salary',\n",
        "            order=company_size_order, errorbar='sd')\n",
        "plt.title('Average Salary by Company Size', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Average Salary ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose the bar chart to visualize the average salary by company size because it is an effective way to compare the average of a numerical variable (avg_salary) across distinct categories (Company Size Category). Bar charts clearly show the magnitude of the average salary for each size category, making it easy to see how salary levels differ based on company size."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Larger companies tend to offer higher average salaries: There is a general trend that as company size increases, the average salary also tends to increase. Companies with 10000+ employees show the highest average salaries.\n",
        "Smaller companies have lower average salaries: Companies in the smaller size categories (e.g., 1 to 50, 51 to 200) generally have lower average salaries compared to larger companies."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "**For Employers:**\n",
        "\n",
        "Talent Acquisition and Retention: Understanding how company size affects salary allows businesses to benchmark their compensation packages against competitors of similar or different sizes. This can help them set competitive salaries to attract and retain talent. If a smaller company knows that larger companies generally pay more, they might need to offer other benefits (e.g., better work-life balance, more opportunities for growth, unique company culture) to compete effectively.\n",
        "Budgeting and Planning: Businesses can use this information for better budgeting and workforce planning, understanding the potential salary costs associated with different growth stages and company sizes.\n",
        "Strategic Decision Making: If a company is considering scaling up, the insights can inform their financial projections and talent strategy related to compensation.\n",
        "\n",
        "\n",
        "**For Job Seekers:**\n",
        "\n",
        "Salary Negotiation: Job seekers can use this information to understand expected salary ranges based on the size of the companies they are applying to, empowering them to negotiate more effectively.\n",
        "Targeting Job Searches: Individuals can refine their job search by targeting companies of certain sizes if salary is a primary factor in their decision-making.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 Geographic Salary"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f251215"
      },
      "source": [
        "import altair as alt\n",
        "\n",
        "# Retrieve 'Location' from the original df for the rows currently in df_clean\n",
        "# This handles cases where rows were removed during outlier treatment, ensuring index alignment.\n",
        "if 'Location' not in df_clean.columns and 'Location' in df.columns:\n",
        "    df_clean['Location'] = df.loc[df_clean.index, 'Location']\n",
        "\n",
        "# Ensure the DataFrame is ready with 'State' and 'avg_salary'\n",
        "if 'State' not in df_clean.columns:\n",
        "    df_clean['State'] = df_clean['Location'].apply(lambda x: x.split(',')[-1].strip() if ',' in x else x)\n",
        "\n",
        "# Mapping of state abbreviations to full names\n",
        "state_mapping = {\n",
        "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
        "    'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
        "    'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas',\n",
        "    'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts',\n",
        "    'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana',\n",
        "    'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico',\n",
        "    'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
        "    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
        "    'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
        "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'District of Columbia' # Added DC\n",
        "}\n",
        "\n",
        "# Create a new column with full state names\n",
        "if 'Full_State' not in df_clean.columns:\n",
        "    df_clean['Full_State'] = df_clean['State'].map(state_mapping)\n",
        "\n",
        "# Calculate top states by average salary using Full_State for grouping and sorting\n",
        "top_states_full = df_clean.groupby('Full_State')['avg_salary'].mean().sort_values(ascending=False).head(15).reset_index()\n",
        "\n",
        "# Create the interactive Altair chart using Full_State\n",
        "chart = alt.Chart(top_states_full).mark_bar().encode(\n",
        "    x=alt.X('avg_salary', title='Average Salary ($)'),\n",
        "    y=alt.Y('Full_State', sort='-x', title='State'),\n",
        "    tooltip=['Full_State', alt.Tooltip('avg_salary', title='Average Salary', format='$,.0f')]\n",
        ").properties(\n",
        "    title='Top 15 States by Average Salary (Interactive - Full Name)'\n",
        ").interactive() # Make the chart interactive\n",
        "\n",
        "# Display the chart\n",
        "chart.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Compare averages across categories: Bar charts are excellent for comparing the mean of a numerical variable (avg_salary) across distinct geographical categories (states).\n",
        "\n",
        "\n",
        "Highlight top performers: By sorting the states by average salary and focusing on the top 15, the bar chart clearly highlights which states offer the highest compensation on average.\n",
        "\n",
        "\n",
        "Provide clear visual ranking: The length of each bar provides an immediate visual comparison and ranking of the average salaries across the selected states."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart clearly shows the top 15 states with the highest average salaries.\n",
        "California appears to have the highest average salary among the states included in the chart.\n",
        "Other states like Illinois, Massachusetts, and New Jersey also show relatively high average salaries.\n",
        "The chart allows for a quick visual comparison of average salaries across these top states, highlighting geographic variations in compensation.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "**For Businesses:**\n",
        "\n",
        "**Talent Acquisition Strategy:** Companies can use this information to understand salary expectations in different locations. This is crucial for setting competitive\n",
        "compensation packages when hiring in various states. If a company is expanding or opening new offices, this insight can inform location decisions based on talent availability and associated salary costs.\n",
        "\n",
        "\n",
        "**Compensation Benchmarking:** Businesses can benchmark their salaries against the average salaries in their specific location and in competitor locations. This helps ensure they are offering competitive wages to attract and retain talent.\n",
        "\n",
        "\n",
        "**Remote Work Policies:** For companies considering remote work options, understanding geographic salary differences can help in defining compensation policies for remote employees based on their location.\n",
        "\n",
        "\n",
        "**Market Analysis:** The chart provides insights into regional salary trends, which can be valuable for market analysis and strategic planning.\n",
        "\n",
        "\n",
        "**For Job Seekers:**\n",
        "\n",
        "**Targeting Job Searches:** Job seekers can use this information to identify states with higher average salaries for their desired roles, helping them focus their job search efforts.\n",
        "\n",
        "\n",
        "**Salary Negotiation:** Knowing the typical salary range in a specific state empowers job seekers to negotiate their salaries more effectively.\n",
        "\n",
        "\n",
        "**Relocation Decisions:** Individuals considering relocation for career opportunities can use this data to understand the potential earning differences in various states."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Industry/Sector Impact on Salaries"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_industries = df_clean.groupby('Industry')['avg_salary'].mean().sort_values(ascending=False).head(10)\n",
        "sns.barplot(x=top_industries.values, y=top_industries.index)\n",
        "plt.title('Top 10 Industries by Average Salary', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Average Salary ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I choose a bar chart to visualize the average salary by industry.\n",
        "\n",
        "**Compare averages across categories:** Bar charts are well-suited for comparing the mean of a numerical variable (avg_salary) across distinct categories (industries).\n",
        "\n",
        "\n",
        "**Highlight top performers:** By sorting the industries by average salary and focusing on the top 10, the bar chart clearly highlights which industries offer the highest compensation on average.\n",
        "\n",
        "\n",
        "**Provide clear visual ranking:** The length of each bar provides an immediate visual comparison and ranking of the average salaries across the selected industries."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "The chart clearly shows the top 10 industries with the highest average salaries.\n",
        "\n",
        "\n",
        "Industries like Other Retail Stores, Motion Picture Production & Distribution, and Financial Analytics & Research appear to have the highest average salaries among the top 10.\n",
        "\n",
        "\n",
        "The chart highlights that average salaries can vary significantly across different industries."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "**For Businesses:**\n",
        "\n",
        "**Talent Acquisition and Retention:** Understanding average salaries across different industries allows companies to benchmark their compensation packages. This is crucial for attracting and retaining talent, especially when competing with companies in higher-paying industries for similar roles.\n",
        "\n",
        "\n",
        "**Industry Competitiveness:** Businesses can assess their salary competitiveness within their own industry and compare it to others. This can inform strategies for attracting talent from different sectors or for positioning themselves as an attractive employer within their industry.\n",
        "\n",
        "\n",
        "**Market Analysis and Strategy:** Insights into industry salary trends are valuable for broader market analysis and strategic planning, including decisions about diversification or focusing on specific industry niches.\n",
        "For Job Seekers:\n",
        "\n",
        "**Targeting Job Searches:** Job seekers can use this information to identify industries that tend to offer higher average salaries for their desired skills and roles, helping them focus their job search.\n",
        "\n",
        "\n",
        "**Salary Negotiation:** Knowing the typical salary range within a specific industry empowers job seekers to negotiate their salaries more effectively.\n",
        "\n",
        "\n",
        "**Career Path Planning:** Understanding salary variations across industries can inform career path decisions and potential transitions between sectors"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Select numerical columns for correlation analysis\n",
        "numerical_columns = ['avg_salary', 'min_salary', 'max_salary', 'Rating', 'sdesc_len', 'num_comp']\n",
        "\n",
        "# Filter only columns that exist in the dataframe\n",
        "existing_numerical = [col for col in numerical_columns if col in df_clean.columns]\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df_clean[existing_numerical].corr()\n",
        "\n",
        "# Create heatmap\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Mask upper triangle\n",
        "heatmap = sns.heatmap(correlation_matrix,\n",
        "                      mask=mask,\n",
        "                      annot=True,\n",
        "                      cmap='coolwarm',\n",
        "                      center=0,\n",
        "                      fmt='.2f',\n",
        "                      square=True,\n",
        "                      cbar_kws={'shrink': 0.8})\n",
        "\n",
        "plt.title('Feature Correlation Heatmap\\n(How Variables Relate to Salary)',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print key correlations with avg_salary\n",
        "print(\"Key Correlations with Average Salary:\")\n",
        "print(\"=\" * 40)\n",
        "if 'avg_salary' in correlation_matrix.columns:\n",
        "    salary_correlations = correlation_matrix['avg_salary'].sort_values(ascending=False)\n",
        "    for feature, corr in salary_correlations.items():\n",
        "        if feature != 'avg_salary':\n",
        "            print(f\"{feature:15} : {corr:+.3f}\")"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f20c35"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7364e9ee"
      },
      "source": [
        "**Answer Here.**\n",
        "\n",
        "**Null Hypothesis (H0):** The average salary is the same across different company size categories.\n",
        "**Alternate Hypothesis (H1):** The average salary is different across different company size categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e269875"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "362db248"
      },
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Drop rows with NaN in 'Company Size Category' or 'avg_salary'\n",
        "df_anova = df_clean.dropna(subset=['Company Size Category', 'avg_salary'])\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "# Create a formula string for the OLS model\n",
        "formula = 'avg_salary ~ C(Q(\"Company Size Category\"))'\n",
        "\n",
        "# Fit the OLS model\n",
        "model = ols(formula, data=df_anova).fit()\n",
        "\n",
        "# Perform ANOVA table calculation\n",
        "anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "\n",
        "# Print the ANOVA table and extract the p-value\n",
        "print(anova_table)\n",
        "p_value_anova = anova_table['PR(>F)'][0]\n",
        "print(f\"\\nANOVA P-value: {p_value_anova:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5d1ade3"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3e52391"
      },
      "source": [
        "**Answer Here.**\n",
        "I have performed a One-Way ANOVA (Analysis of Variance) test using `statsmodels.api` to obtain the P-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88995841"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30342603"
      },
      "source": [
        "**Answer Here.**\n",
        "I chose the One-Way ANOVA test because I am comparing the means of a continuous variable (average salary) across three or more independent groups (different company size categories). ANOVA is used to determine if there are any statistically significant differences between the means of these groups."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values before handling:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "\n",
        "# Let's re-check for NaNs after our wrangling steps\n",
        "print(\"\\nMissing values after initial wrangling:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# We will fill the NaN salary values with the median of the average salary.\n",
        "median_avg_salary = df_clean['avg_salary'].median()\n",
        "df_clean['avg_salary'] = df_clean['avg_salary'].fillna(median_avg_salary)\n",
        "df_clean['min_salary'] = df_clean['min_salary'].fillna(df_clean['min_salary'].median())\n",
        "df_clean['max_salary'] = df_clean['max_salary'].fillna(df_clean['max_salary'].median())\n",
        "\n",
        "\n",
        "\n",
        "# Let's replace -1 in 'Founded' with NaN and then impute with the median year.\n",
        "df_clean['Founded'] = df_clean['Founded'].replace(-1, np.nan)\n",
        "median_founded_year = df_clean['Founded'].median()\n",
        "df_clean['Founded'] = df_clean['Founded'].fillna(median_founded_year)\n",
        "\n",
        "\n",
        "\n",
        "# Re-check for missing values after imputation\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "**Median Imputation for Numerical Columns ('min_salary', 'max_salary', 'avg_salary', 'Founded')**-I chose to impute the missing values in these numerical columns with the median. The median is a robust measure of central tendency that is less affected by outliers compared to the mean. This is particularly useful for salary data, which can sometimes have extreme values. For the 'Founded' column, replacing -1 (which likely represents missing information) with the median year provides a reasonable estimate for companies with unknown founding dates, assuming the distribution of founding years is not heavily skewed."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Visualize numerical columns to identify outliers\n",
        "numerical_cols = ['avg_salary', 'Rating', 'Founded', 'num_comp']\n",
        "\n",
        "# Filter for columns that exist in the DataFrame\n",
        "existing_numerical_cols = [col for col in numerical_cols if col in df_clean.columns]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(existing_numerical_cols):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    sns.boxplot(y=df_clean[col])\n",
        "    plt.title(f'Box plot of {col}', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# You can also use descriptive statistics to identify potential outliers\n",
        "print(\"\\nDescriptive statistics of numerical columns:\")\n",
        "print(df_clean[existing_numerical_cols].describe())\n",
        "\n",
        "\n",
        "# Identify outliers using the IQR method for 'avg_salary':\n",
        "Q1 = df_clean['avg_salary'].quantile(0.25)\n",
        "Q3 = df_clean['avg_salary'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"\\nIQR for Average Salary:\")\n",
        "print(f\"  Q1: {Q1:.2f}\")\n",
        "print(f\"  Q3: {Q3:.2f}\")\n",
        "print(f\"  IQR: {IQR:.2f}\")\n",
        "print(f\"  Lower Bound (for outliers): {lower_bound:.2f}\")\n",
        "print(f\"  Upper Bound (for outliers): {upper_bound:.2f}\")\n",
        "\n",
        "# Remove outliers based on IQR for 'avg_salary'\n",
        "df_clean_filtered = df_clean[(df_clean['avg_salary'] >= lower_bound) & (df_clean['avg_salary'] <= upper_bound)].copy()\n",
        "\n",
        "print(f\"\\nShape before outlier removal: {df_clean.shape}\")\n",
        "print(f\"Shape after outlier removal (avg_salary): {df_clean_filtered.shape}\")\n",
        "\n",
        "# Update df_clean to the filtered version\n",
        "df_clean = df_clean_filtered\n",
        "\n",
        "print(\"\\nMissing values after outlier removal:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "***IQR Method for avg_salary:*** I used the Interquartile Range (IQR) method to identify and remove outliers in the avg_salary column. The IQR is the range between the first quartile (Q1) and the third quartile (Q3). Outliers are typically defined as values that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR.\n",
        "\n",
        "I chose this method because the box plots showed clear potential outliers in the average salary distribution. The IQR method is a common and effective way to handle outliers in numerical data, as it is less sensitive to extreme values than methods that rely on the mean and standard deviation. By removing these outliers, we can improve the performance of our machine learning models by reducing the impact of extreme values on the training process.\n",
        "\n",
        "I also visualized the numerical columns using box plots and printed descriptive statistics to get a better understanding of the data distribution and identify potential outliers before applying the IQR method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8831d22"
      },
      "source": [
        "### Removing Unwanted Columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical Encoding\n",
        "# Identify categorical columns\n",
        "print(\"Categorical Variables Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "categorical_columns = []\n",
        "# Define a list of columns to exclude from the nunique check\n",
        "exclude_cols = ['Job Description', 'description_tokens', 'description_lemmatized', 'description_pos_tags', 'description_lemmatized_str']\n",
        "\n",
        "for col in df_clean.columns:\n",
        "    # Exclude columns that are in the exclude_cols list\n",
        "    if col not in exclude_cols:\n",
        "        if df_clean[col].dtype == 'object' or df_clean[col].nunique() < 20:\n",
        "            unique_count = df_clean[col].nunique()\n",
        "            categorical_columns.append(col)\n",
        "            print(f\"{col:25} : {unique_count:3} unique values\")\n",
        "            if unique_count <= 10:  # Show values for columns with few categories\n",
        "                print(f\"{'':25}   {df_clean[col].unique()}\")\n",
        "\n",
        "print(f\"\\nTotal categorical columns: {len(categorical_columns)}\")\n",
        "\n",
        "# Key categorical features for encoding\n",
        "key_categorical = ['Job Role', 'Company Size Category', 'State', 'Industry', 'Sector', 'Type of ownership']\n",
        "# Filter to only include columns that exist in our dataframe\n",
        "key_categorical = [col for col in key_categorical if col in df_clean.columns and col not in exclude_cols]\n",
        "\n",
        "print(f\"\\nKey categorical features for encoding: {key_categorical}\")"
      ],
      "metadata": {
        "id": "fEbQkc8Lj5Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1912f83b"
      },
      "source": [
        "# Apply one-hot encoding to key categorical features\n",
        "key_categorical = ['Job Role', 'Company Size Category', 'State', 'Industry', 'Sector', 'Type of ownership']\n",
        "\n",
        "# Ensure columns exist before encoding\n",
        "existing_key_categorical = [col for col in key_categorical if col in df_clean.columns]\n",
        "\n",
        "df_encoded = pd.get_dummies(df_clean, columns=existing_key_categorical, drop_first=True) # drop_first=True to avoid multicollinearity\n",
        "\n",
        "# Display the head of the encoded DataFrame\n",
        "display(df_encoded.head())\n",
        "\n",
        "print(f\"\\nShape of DataFrame after One-Hot Encoding: {df_encoded.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Machine learning algorithms typically work with numerical data, so categorical variables need to be converted into a numerical format.\n",
        "\n",
        "\n",
        "The choice of technique depends on the nature of the categorical variable (nominal or ordinal) and the cardinality (number of unique categories).\n",
        "\n",
        "\n",
        "One-hot encoding is a simple and effective method for nominal variables.\n",
        "\n",
        "\n",
        "Target encoding can be beneficial for high-cardinality features, but it's important to use cross-validation or proper splitting to prevent data leakage."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lower_casing(text):\n",
        "  # Lower Casing\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "  # Remove Punctuations\n",
        "  text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "  return text"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "def remove_urls(text):\n",
        "  # Remove URLs\n",
        "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "  text = url_pattern.sub(r'', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespaces(text):\n",
        "  # Remove White spaces\n",
        "  text = text.strip()\n",
        "  return text"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8dfbdb6"
      },
      "source": [
        "# Define the tokenization function\n",
        "def tokenize_text(text):\n",
        "  # Tokenization\n",
        "  return nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af82620f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b38855a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "170615c6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cc981a6"
      },
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "# Import necessary libraries for lemmatization\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download necessary NLTK data if not already downloaded\n",
        "try:\n",
        "    wordnet.ensure_loaded()\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the lemmatization function to the 'description_tokens' column\n",
        "df_clean['description_lemmatized'] = df_clean['description_tokens'].apply(lemmatize_tokens)\n",
        "\n",
        "# Display the head of the updated DataFrame\n",
        "display(df_clean.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "985ea62a"
      },
      "source": [
        "Answer Here.\n",
        "I have used **Lemmatization** as the text normalization technique.\n",
        "\n",
        "**Why Lemmatization?**\n",
        "\n",
        "Lemmatization is a more sophisticated technique than stemming because it reduces words to their base or dictionary form (lemma). Unlike stemming, which simply chops off prefixes or suffixes and can result in non-words, lemmatization considers the context and converts the word to its meaningful base form. For example, 'running' becomes 'run', 'better' becomes 'good' etc.\n",
        "\n",
        "In the context of analyzing job descriptions, lemmatization helps in grouping together different forms of the same word (e.g., \"develop,\" \"developing,\" \"developed\" all become \"develop\"). This is important for accurate feature extraction and analysis, as it ensures that variations of a word are treated as the same concept. This leads to a more accurate representation of the vocabulary and can improve the performance of downstream tasks like text vectorization and model building.\n",
        "\n",
        "While stemming is faster, the slightly increased computational cost of lemmatization is often offset by the improved quality of the normalized text, especially for tasks that require a deeper understanding of word meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8582d872"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "482345b8"
      },
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "\n",
        "# Download the averaged_perceptron_tagger if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError: # Corrected exception handling\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Define a function to perform POS tagging\n",
        "def pos_tagging(tokens):\n",
        "    # Ensure tokens is a list before tagging\n",
        "    if isinstance(tokens, list):\n",
        "      return nltk.pos_tag(tokens)\n",
        "    else:\n",
        "      return [] # Return empty list for non-list input\n",
        "\n",
        "\n",
        "# Check if 'description_lemmatized' exists before applying POS tagging\n",
        "if 'description_lemmatized' in df_clean.columns:\n",
        "    # Apply the pos_tagging function to the 'description_lemmatized' column\n",
        "    df_clean['description_pos_tags'] = df_clean['description_lemmatized'].apply(pos_tagging)\n",
        "\n",
        "    # Display the head of the updated DataFrame\n",
        "    display(df_clean.head())\n",
        "else:\n",
        "    print(\"Error: 'description_lemmatized' column not found. Please ensure lemmatization was successful.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8577e137"
      },
      "source": [
        "# Display the first few entries of the 'description_pos_tags' column\n",
        "display(df_clean['description_pos_tags'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86abf5bc"
      },
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Check if 'description_lemmatized' column exists before proceeding\n",
        "if 'description_lemmatized' not in df_clean.columns:\n",
        "    print(\"Error: 'description_lemmatized' column not found.\")\n",
        "    tfidf_matrix = None # Set to None to indicate vectorization could not proceed\n",
        "else:\n",
        "    # Join the lemmatized tokens back into strings for TF-IDF\n",
        "    df_clean['description_lemmatized_str'] = df_clean['description_lemmatized'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    # We can set a max_features to limit the vocabulary size and reduce dimensionality\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "    # Fit and transform the lemmatized text data\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_clean['description_lemmatized_str'])\n",
        "\n",
        "    # Display the shape of the resulting TF-IDF matrix\n",
        "    print(\"Shape of TF-IDF matrix:\", tfidf_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df733b0a"
      },
      "source": [
        "Answer Here.\n",
        "I have used **TF-IDF (Term Frequency-Inverse Document Frequency)** as the text vectorization technique.\n",
        "\n",
        "**Why TF-IDF?**\n",
        "\n",
        "TF-IDF is a widely used and effective technique for converting text into numerical vectors, especially when the goal is to represent the importance of words within documents relative to a whole corpus. Here's why it's a good choice for analyzing job descriptions:\n",
        "\n",
        "*   **Captures Term Importance:** TF-IDF assigns a weight to each term based on its frequency within a specific document (Term Frequency - TF) and its inverse frequency across all documents (Inverse Document Frequency - IDF). This means words that are frequent in a particular job description but rare across all job descriptions will have a higher TF-IDF score, indicating their potential importance in distinguishing that job description.\n",
        "*   **Reduces the Impact of Common Words:** Common words like \"the,\" \"a,\" \"is\" (stopwords) will have a low IDF score because they appear in many documents. This reduces their weight, preventing them from dominating the vector representation.\n",
        "*   **Handles Document Length Variations:** TF-IDF inherently accounts for the length of documents, preventing longer documents from having disproportionately larger vector values.\n",
        "\n",
        "In the context of salary prediction from job descriptions, TF-IDF helps in identifying the most relevant keywords and phrases that might influence salary levels (e.g., specific skills, technologies, or responsibilities mentioned frequently in higher-paying jobs but less so in lower-paying ones).\n",
        "\n",
        "While other techniques like Count Vectorization (simple word counts) or Word Embeddings exist, TF-IDF strikes a good balance between simplicity, interpretability, and effectiveness for many text classification and regression tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Create a feature for job description length\n",
        "df_clean['description_len'] = df_clean['Job Description'].apply(len)\n",
        "\n",
        "# Create a simplified job title feature\n",
        "def simplify_title(title):\n",
        "    if 'data scientist' in title.lower():\n",
        "        return 'data scientist'\n",
        "    elif 'data engineer' in title.lower():\n",
        "        return 'data engineer'\n",
        "    elif 'analyst' in title.lower():\n",
        "        return 'analyst'\n",
        "    elif 'manager' in title.lower():\n",
        "        return 'manager'\n",
        "    elif 'director' in title.lower():\n",
        "        return 'manager'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "df_clean['simple_title'] = df_clean['Job Title'].apply(simplify_title)\n",
        "\n",
        "# Extract state from location\n",
        "df_clean['job_state'] = df_clean['Location'].apply(lambda x: x.split(',')[-1].strip() if ',' in x else x)\n",
        "\n",
        "# You can add more feature manipulation steps here based on your analysis and domain knowledge.\n",
        "\n",
        "# Display the head of the DataFrame with new features\n",
        "display(df_clean.head())"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a01261"
      },
      "source": [
        "## Visualize feature relationships\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac096a4a"
      },
      "source": [
        "# Chart - 14 - Correlation Heatmap (Re-using the existing chart ID)\n",
        "plt.figure(figsize=(10, 8))\n",
        "# Select numerical columns for correlation analysis including the target variable\n",
        "numerical_cols_for_corr = ['avg_salary', 'Rating', 'Founded', 'num_comp', 'description_len']\n",
        "\n",
        "# Filter only columns that exist in the dataframe and are numeric\n",
        "existing_numerical_for_corr = [col for col in numerical_cols_for_corr if col in df_clean.columns and pd.api.types.is_numeric_dtype(df_clean[col])]\n",
        "\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix_subset = df_clean[existing_numerical_for_corr].corr()\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(correlation_matrix_subset,\n",
        "            annot=True,\n",
        "            cmap='coolwarm',\n",
        "            center=0,\n",
        "            fmt='.2f',\n",
        "            square=True,\n",
        "            cbar_kws={'shrink': 0.8})\n",
        "\n",
        "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Chart - 15 - Pair Plot (Re-using the existing chart ID)\n",
        "# Select a subset of relevant numerical features for pair plot\n",
        "# Based on the correlation heatmap and domain knowledge, choose features that show some variation and potential relation to salary\n",
        "pairplot_cols = ['avg_salary', 'Rating', 'num_comp', 'description_len']\n",
        "\n",
        "# Filter for columns that exist in the dataframe\n",
        "existing_pairplot_cols = [col for col in pairplot_cols if col in df_clean.columns and pd.api.types.is_numeric_dtype(df_clean[col])]\n",
        "\n",
        "\n",
        "if len(existing_pairplot_cols) > 1:\n",
        "    sns.pairplot(df_clean[existing_pairplot_cols])\n",
        "    plt.suptitle('Pair Plot of Selected Numerical Features', y=1.02, fontsize=16, fontweight='bold')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Not enough numerical columns available for pair plot.\")\n",
        "\n",
        "# Additional visualization: Box plot for top industries vs avg_salary\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Recalculate top industries from the potentially filtered df_clean\n",
        "top_industries = df_clean.groupby('Industry')['avg_salary'].mean().sort_values(ascending=False).head(10)\n",
        "# Filter df_clean to include only these top industries\n",
        "df_top_industries = df_clean[df_clean['Industry'].isin(top_industries.index)]\n",
        "sns.boxplot(data=df_top_industries, x='Industry', y='avg_salary')\n",
        "plt.title('Average Salary Distribution in Top 10 Industries', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Average Salary ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional visualization: Box plot for company size vs avg_salary\n",
        "plt.figure(figsize=(10, 6))\n",
        "# Define the order for company size categories\n",
        "company_size_order_viz = ['1 to 50', '51 to 200', '201 to 500',\n",
        "                          '501 to 1000', '1001 to 5000', '5001 to 10000', '10000+', 'Unknown']\n",
        "# Filter df_clean to include only valid company size categories\n",
        "df_valid_size = df_clean[df_clean['Company Size Category'].isin(company_size_order_viz)]\n",
        "sns.boxplot(data=df_valid_size, x='Company Size Category', y='avg_salary',\n",
        "            order=company_size_order_viz)\n",
        "plt.title('Average Salary Distribution by Company Size', fontsize=14, fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Average Salary ($)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Discussion on Domain Knowledge and Feature Selection\n",
        "Based on real-world understanding of the job market, several features are intuitively expected to be strong predictors of salary:\n",
        "\n",
        "Job Title: This is arguably the most important factor. Different job roles have vastly different salary ranges based on required skills, responsibilities, and market demand (e.g., a Senior Data Scientist typically earns more than a Junior Data Analyst). Our ANOVA test confirmed that 'Job Role' has a highly significant impact on average salary (p-value << 0.05).\n",
        "\n",
        "Experience Level: While not a direct column in our dataset, experience is often implied in job titles (e.g., \"Senior\", \"Lead\"). More experienced professionals generally command higher salaries due to their expertise and proven track record. This is a crucial factor that would ideally be explicitly engineered or extracted from the 'Job Description'.\n",
        "\n",
        "Location: Salaries vary significantly based on the cost of living and demand in different geographic areas. Major tech hubs or cities with high costs of living typically offer higher salaries for the same role compared to rural areas. Our geographic salary visualization and ANOVA on 'State' (though not explicitly shown in the summary, the p-value was very low in the previous run) support this.\n",
        "\n",
        "Required Skills: Specific technical skills (e.g., Python, SQL, Machine Learning frameworks, Cloud platforms) and soft skills are highly valued and directly impact earning potential. Jobs requiring specialized or in-demand skills tend to pay more. This information is primarily embedded in the 'Job Description' and was partially captured through TF-IDF vectorization, which can highlight important skill keywords.\n",
        "\n",
        "Company Size: Larger companies often have more structured compensation plans, potentially higher revenue to support higher salaries, and more complex projects requiring specialized skills. Our bar chart and ANOVA confirmed that 'Company Size Category' has a statistically significant relationship with average salary.\n",
        "\n",
        "Industry and Sector: Different industries and sectors have varying levels of profitability, demand for specific roles, and established salary norms. For instance, tech, finance, or pharmaceuticals might offer higher salaries for data professionals compared to non-profit or education sectors. Our bar charts and ANOVA for 'Industry' and 'Sector' showed statistically significant differences in average salaries across categories.\n",
        "\n",
        "Company Name and Rating: Reputable or highly-rated companies might attract top talent by offering competitive salaries and better benefits. Company-specific factors can influence compensation. Our correlation analysis showed a weak positive correlation between 'Rating' and 'avg_salary', suggesting a minor influence.\n",
        "\n",
        "Type of Ownership: The ownership structure (e.g., public, private, government, non-profit) can influence compensation strategies and salary levels. Our ANOVA confirmed 'Type of ownership' is statistically significant.\n",
        "\n",
        "Revenue: A company's revenue directly relates to its ability to pay employees. Higher revenue companies are generally expected to offer higher salaries. Although 'Revenue' was not included in the filter methods summary here, it's a strong candidate based on domain knowledge and would likely show significance in statistical tests or importance in model-based methods.\n",
        "\n",
        "Comparison with Filter Method Results:\n",
        "\n",
        "The filter methods (correlation and ANOVA) largely confirmed our domain knowledge expectations. 'Job Role', 'Company Size Category', 'Industry', 'Sector', and 'Type of ownership' were all identified as statistically significant predictors of avg_salary through ANOVA. Numerical features like 'Rating' and 'num_comp' showed weak positive correlations, indicating a less pronounced linear relationship but still potentially relevant. The high correlation between 'min_salary', 'max_salary', and 'avg_salary' is expected as they represent the same underlying salary information.\n",
        "\n",
        "While filter methods provide initial insights based on individual feature relationships, domain knowledge helps us prioritize which features to engineer or focus on further, especially for complex information like skills and experience embedded in text data. It also guides the interpretation of statistical results and helps identify potential limitations (e.g., 'Founded' having a weak correlation might be due to how the data is distributed or interactions with other factors)."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsEC7HUjK7v3"
      },
      "source": [
        "**Justify Chosen Features**\n",
        "\n",
        "Based on the analysis performed using filter methods (correlation analysis and ANOVA) and our domain knowledge of the job market, the following features were chosen for predicting average salary:\n",
        "\n",
        "*   **Job Role:** The ANOVA test showed a highly statistically significant difference in average salary across different job roles (Data Scientist, Data Engineer, Analyst, Manager, Other). This aligns with domain knowledge, as job titles directly reflect the level of expertise, responsibility, and market demand, which are major determinants of salary.\n",
        "*   **Company Size Category:** The ANOVA test indicated a statistically significant difference in average salary based on company size. Our bar chart visualization also showed a clear trend of increasing average salary with increasing company size. Larger companies often have more resources and structured compensation scales, supporting this finding.\n",
        "*   **State:** The ANOVA test revealed a statistically significant difference in average salary across different states. This confirms the domain knowledge that geographic location significantly impacts salary due to variations in cost of living, industry concentration, and local market demand.\n",
        "*   **Industry and Sector:** Both Industry and Sector showed statistically significant differences in average salary according to the ANOVA tests. Different industries and sectors have varying levels of profitability, demand for specific skills, and established compensation norms, which influences salary ranges.\n",
        "*   **Type of ownership:** The ANOVA test indicated a statistically significant difference in average salary based on the type of company ownership. This suggests that factors related to the ownership structure (e.g., public vs. private, non-profit) can influence compensation practices.\n",
        "*   **Rating:** The correlation analysis showed a weak positive linear correlation between company rating and average salary. While not a strong individual predictor based on this analysis, domain knowledge suggests that reputable companies (often reflected in higher ratings) might offer slightly better compensation to attract talent. It was included as a potentially contributing factor.\n",
        "*   **Founded:** The correlation analysis showed a very weak linear correlation with average salary. However, domain knowledge suggests that the age of a company might indirectly relate to its stability, maturity, and potentially its compensation structure. It was kept for this reason, although its individual predictive power appears low based on the filter method.\n",
        "*   **num_comp:** The correlation analysis showed a very weak positive linear correlation with average salary. While the direct linear relationship is weak, the number of competitors could potentially influence salary in complex ways (e.g., highly competitive markets might drive salaries up). It was retained as a feature that might contribute in combination with others.\n",
        "*   **description_len:** The correlation analysis showed a weak positive linear correlation with average salary. Longer job descriptions might indicate more complex roles or detailed requirements, which could be associated with higher salaries.\n",
        "\n",
        "**Features Excluded (and why):**\n",
        "\n",
        "*   **min_salary and max_salary:** These were excluded because they are direct components used to calculate `avg_salary`, and including them would lead to perfect multicollinearity and prevent the model from learning the influence of other features.\n",
        "\n",
        "*   **Job Title (original), Salary Estimate, Company Name, Location, Headquarters, Size, Revenue, Competitors (original string):** These original columns were either transformed (e.g., Salary Estimate into min/max/avg salary, Size into Company Size Category, Location into State) or are high-cardinality text/identifier columns that are better represented by engineered features (like `description_len`, `simple_title`, or information extracted from the job description text via TF-IDF, although TF-IDF features were not explicitly included in the Linear Regression model evaluation here for simplicity, they are valuable for capturing skills/requirements).\n",
        "\n",
        "The selected features represent a balance between statistical significance identified by filter methods and the intuitive importance based on domain knowledge. This set aims to capture the most influential factors determining job salaries in this dataset. Further model-based feature selection (wrapper or embedded methods) could be applied to refine this set and potentially improve model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "import scipy.sparse\n",
        "\n",
        "target = 'avg_salary'\n",
        "\n",
        "numerical_cols_for_X = [\n",
        "    'Rating',\n",
        "    'Founded',\n",
        "    'num_comp',\n",
        "    'description_len' # Created in Feature Manipulation\n",
        "]\n",
        "\n",
        "existing_numerical_cols_for_X = [col for col in numerical_cols_for_X if col in df_clean.columns]\n",
        "\n",
        "X_numerical = df_clean[existing_numerical_cols_for_X].copy()\n",
        "\n",
        "X_numerical = X_numerical.fillna(X_numerical.median())\n",
        "\n",
        "\n",
        "\n",
        "# Get the list of one-hot encoded columns corresponding to the selected categorical features\n",
        "categorical_features_used = ['Job Role', 'Company Size Category', 'State', 'Industry', 'Sector', 'Type of ownership']\n",
        "\n",
        "# Filter columns in df_encoded that start with any of the categorical_features_used names\n",
        "X_categorical = df_encoded.loc[:, df_encoded.columns.str.startswith(tuple(cat + '_' for cat in categorical_features_used))]\n",
        "\n",
        "\n",
        "# Prepare text features (TF-IDF matrix)\n",
        "X_text = tfidf_matrix\n",
        "\n",
        "\n",
        "# Convert X_numerical and X_categorical to sparse matrices if they are not already, for efficient concatenation with X_text\n",
        "X_numerical_sparse = scipy.sparse.csr_matrix(X_numerical.values)\n",
        "X_categorical_sparse = scipy.sparse.csr_matrix(X_categorical.values)\n",
        "\n",
        "# Concatenate all sparse matrices horizontally\n",
        "X = scipy.sparse.hstack([\n",
        "    X_numerical_sparse,\n",
        "    X_categorical_sparse,\n",
        "    X_text\n",
        "])\n",
        "\n",
        "# Define the target variable y\n",
        "y = df_encoded[target]\n",
        "\n",
        "print(f\"Shape of combined feature matrix X: {X.shape}\")\n",
        "print(f\"Shape of target variable y: {y.shape}\")\n",
        "\n",
        "print(\"First 5 rows of y:\")\n",
        "display(y.head())"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "\n",
        "Transformation of data is important for converting the raw data in the usable format like in terms of analysing reporting etc. Doing this is very important because of cleaning, restructuring, and enriching the data to improve its quality, ensure compatibility with target systems, and reveal underlying patterns and insights.\n",
        "\n",
        "I have used categorical encoding and text vectorization.\n",
        "\n"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "# Re-define numerical features that need scaling\n",
        "numerical_cols_to_scale = [\n",
        "    'Rating',\n",
        "    'Founded',\n",
        "    'num_comp',\n",
        "    'description_len'\n",
        "]\n",
        "\n",
        "# Create a copy of the numerical features part of df_clean before scaling\n",
        "X_numerical_to_scale = df_clean[numerical_cols_to_scale].copy()\n",
        "\n",
        "# Impute any remaining NaNs (as done before, just to be safe)\n",
        "X_numerical_to_scale = X_numerical_to_scale.fillna(X_numerical_to_scale.median())\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the numerical features\n",
        "X_numerical_scaled = scaler.fit_transform(X_numerical_to_scale)\n",
        "\n",
        "# Convert the scaled numerical features back to a DataFrame for easier handling or directly to sparse\n",
        "X_numerical_scaled_sparse = scipy.sparse.csr_matrix(X_numerical_scaled)\n",
        "\n",
        "# Re-use X_categorical_sparse and X_text (tfidf_matrix) from previous steps\n",
        "\n",
        "# Re-concatenate all sparse matrices horizontally to form the final scaled X\n",
        "X_scaled = scipy.sparse.hstack([\n",
        "    X_numerical_scaled_sparse,\n",
        "    X_categorical_sparse,\n",
        "    X_text\n",
        "])\n",
        "\n",
        "print(\"Data scaling completed for numerical features within the combined matrix.\")\n",
        "print(f\"Shape of scaled combined feature matrix X_scaled: {X_scaled.shape}\")\n",
        "\n",
        "# Assign X_scaled back to X for consistency with subsequent steps\n",
        "X = X_scaled\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e30e285"
      },
      "source": [
        "Answer Here.\n",
        "\n",
        "I have used **StandardScaler** to scale the numerical features. Here's why:\n",
        "\n",
        "1.  **Standardization:** StandardScaler transforms the data such that its mean is 0 and its standard deviation is 1. This process is called standardization.\n",
        "2.  **Algorithm Performance:** Many machine learning algorithms (like Linear Regression, SVMs, neural networks, or algorithms that rely on distance calculations) perform better or converge faster when numerical input features are on a similar scale. Features with larger values might disproportionately influence the model's objective function.\n",
        "3.  **Preserves Distribution:** Unlike normalization (MinMaxScaler) which scales features to a fixed range (e.g., 0 to 1), standardization doesn't bound values to a specific range. It's less affected by outliers, as it doesn't compress all values into a small interval, which can be beneficial if the data has some outliers that we chose not to remove entirely.\n",
        "4.  **Sparse Matrix Compatibility:** For our combined feature matrix `X`, which contains a mix of scaled numerical features, one-hot encoded categorical features (which are already 0s and 1s and don't need scaling in the same way), and TF-IDF features (which are also often sparse and have their own scaling properties), applying `StandardScaler` only to the numerical components before concatenation is a clean and effective strategy. This avoids trying to scale the 0/1 values of one-hot encoded features or the TF-IDF scores unnecessarily, which could degrade model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# The number of components can be tuned. Starting with 500 as a reasonable reduction.\n",
        "# Ensure n_components is less than min(n_samples, n_features)\n",
        "num_components = min(500, X.shape[0] - 1, X.shape[1] - 1)\n",
        "\n",
        "# Initialize TruncatedSVD\n",
        "svd = TruncatedSVD(n_components=num_components, random_state=42)\n",
        "\n",
        "# Fit and transform the combined feature matrix X\n",
        "X_reduced = svd.fit_transform(X)\n",
        "\n",
        "print(f\"Original feature matrix shape: {X.shape}\")\n",
        "print(f\"Reduced feature matrix shape: {X_reduced.shape}\")\n",
        "\n",
        "# Update X to the reduced version for subsequent steps\n",
        "X = X_reduced"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I used Truncated SVD (Singular Value Decomposition) for dimensionality reduction. This technique is well-suited for high-dimensional, sparse datasets like ours (which includes TF-IDF features) because it effectively reduces the number of features while preserving the most significant variance in the data. It helps in mitigating the 'curse of dimensionality', improving model performance by reducing overfitting and computational costs, and also helps in noise reduction."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# X is the feature matrix (already reduced in dimensionality)\n",
        "# y is the target variable (average salary)\n",
        "# test_size=0.2 means 20% of the data will be used for testing, and 80% for training\n",
        "# random_state ensures reproducibility of the split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you use and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Split the data into training and testing sets X is the feature matrix (already reduced in dimensionality)\n",
        "y is the target variable (average salary)\n",
        "test_size=0.2 means 20% of the data will be used for testing, and 80% for training\n",
        "random_state ensures reproducibility of the split"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33f29a7b"
      },
      "source": [
        "Answer Here.\n",
        "\n",
        "To determine if the salary data is imbalanced, we need to examine the distribution of the `avg_salary` target variable. For continuous variables like salary, 'imbalance' isn't about unequal class representation but rather about whether the data is heavily skewed or concentrated in specific ranges, which can affect model training.\n",
        "\n",
        "Let's visualize the distribution and look at some descriptive statistics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_clean['avg_salary'], kde=True, bins=30)\n",
        "plt.title('Distribution of Average Salary')\n",
        "plt.xlabel('Average Salary ($)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDescriptive Statistics for Average Salary:\")\n",
        "print(df_clean['avg_salary'].describe())\n",
        "\n",
        "print(\"\\nSkewness of Average Salary:\")\n",
        "print(df_clean['avg_salary'].skew())"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Based on my analysis of the avg_salary distribution, the dataset for this continuous target variable does not appear to be significantly imbalanced. The histogram showed a relatively normal distribution, and the skewness value was close to zero (0.174), indicating a fairly symmetrical spread of salaries. Therefore, no specific technique was needed or applied to balance the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation: Linear Regression\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X (features) and y (target) have been defined and split in previous steps\n",
        "# If not, you would need to add the splitting code here as well.\n",
        "# For now, let's assume X_train, X_test, y_train, y_test are available from cell c377d1d9\n",
        "\n",
        "# Initialize the Linear Regression model\n",
        "linear_reg_model = LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "linear_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_lr = linear_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "rmse_lr = np.sqrt(mse_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Linear Regression Model Performance:\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_lr:.2f}\")\n",
        "print(f\"  R-squared (R2): {r2_lr:.4f}\")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Evaluation metrics from the Linear Regression model\n",
        "metrics = {'Metric': ['MAE', 'MSE', 'RMSE', 'R-squared'],\n",
        "           'Score': [mae_lr, mse_lr, rmse_lr, r2_lr]}\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# For visualization purposes, especially with varying scales,\n",
        "# we might want to plot MAE, RMSE, and R-squared separately or use a log scale for errors.\n",
        "# Let's create two plots: one for error metrics (MAE, RMSE) and one for R-squared.\n",
        "\n",
        "# Plotting Error Metrics (MAE and RMSE)\n",
        "error_metrics_df = metrics_df[metrics_df['Metric'].isin(['MAE', 'RMSE'])]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=error_metrics_df)\n",
        "plt.title('Linear Regression Model Error Metrics', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# Plotting R-squared\n",
        "r2_metric_df = metrics_df[metrics_df['Metric'] == 'R-squared']\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x='Metric', y='Score', data=r2_metric_df)\n",
        "plt.title('Linear Regression Model R-squared Score', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1) # R-squared is typically between 0 and 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1: Cross-Validation for Linear Regression\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform k-fold cross-validation (e.g., k=5)\n",
        "# We will use the Root Mean Squared Error (RMSE) as the scoring metric,\n",
        "# as it's commonly used for regression tasks and is in the same units as the target variable.\n",
        "# cross_val_score returns negative MSE, so we take the absolute value and then the square root for RMSE.\n",
        "# We also negate the score because cross_val_score expects a scoring function where higher is better.\n",
        "# For negative MSE, higher (less negative) is better.\n",
        "\n",
        "# Define scoring as negative MSE\n",
        "scoring = 'neg_mean_squared_error'\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(linear_reg_model, X, y, cv=5, scoring=scoring)\n",
        "\n",
        "# Convert negative MSE scores to positive MSE and then to RMSE\n",
        "cv_rmse_scores = np.sqrt(-cv_scores)\n",
        "\n",
        "print(\"Cross-Validation Results (RMSE):\")\n",
        "print(f\"  Individual Fold RMSEs: {cv_rmse_scores}\")\n",
        "print(f\"  Mean Cross-Validation RMSE: {cv_rmse_scores.mean():.2f}\")\n",
        "print(f\"  Standard Deviation of CV RMSE: {cv_rmse_scores.std():.2f}\")\n",
        "\n",
        "# You can also calculate other metrics if needed\n",
        "# cv_mae_scores = -cross_val_score(linear_reg_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
        "# print(f\"  Mean Cross-Validation MAE: {cv_mae_scores.mean():.2f}\")\n",
        "\n",
        "# cv_r2_scores = cross_val_score(linear_reg_model, X, y, cv=5, scoring='r2')\n",
        "# print(f\"  Mean Cross-Validation R2: {cv_r2_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ea22030"
      },
      "source": [
        "Answer Here.\n",
        "For the standard Linear Regression model, there are no hyperparameters to tune using techniques like Grid Search or Random Search. Linear Regression directly calculates the coefficients that minimize the sum of squared errors based on the provided data.\n",
        "\n",
        "The cross-validation performed in the previous step is used to assess the model's performance consistency across different subsets of the data, giving a more reliable estimate of how well it might generalize to unseen data, rather than optimizing internal model parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "880ed486"
      },
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Since we performed cross-validation instead of hyperparameter tuning for the standard Linear Regression model, there isn't an \"improvement\" in the model itself in terms of optimized parameters.\n",
        "\n",
        "However, the cross-validation results provide a more reliable estimate of how the model is likely to perform on unseen data compared to a single train-test split. The mean cross-validation RMSE (Root Mean Squared Error) gives us a better understanding of the model's typical prediction error across different subsets of the data.\n",
        "\n",
        "We can note the mean cross-validation RMSE as a more robust performance indicator than the single split RMSE (approximately {{cv_rmse_scores.mean():.2f}} based on the previous output). There isn't a separate chart to update in this case, as we didn't tune hyperparameters to get a new set of scores for a modified model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation: Random Forest Regressor\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the Random Forest Regressor model\n",
        "# You can start with default parameters or some initial values\n",
        "rf_reg_model = RandomForestRegressor(n_estimators=100, random_state=42) # n_estimators is a common parameter to tune\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_reg_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_rf = rf_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mse_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Random Forest Regressor Model Performance (Untuned):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_rf:.2f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_rf:.2f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_rf:.2f}\")\n",
        "print(f\"  R-squared (R2): {r2_rf:.4f}\")\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Evaluation metrics from the Random Forest Regressor model\n",
        "metrics_rf = {'Metric': ['MAE', 'MSE', 'RMSE', 'R-squared'],\n",
        "              'Score': [mae_rf, mse_rf, rmse_rf, r2_rf]}\n",
        "metrics_rf_df = pd.DataFrame(metrics_rf)\n",
        "\n",
        "# Plotting Error Metrics (MAE and RMSE)\n",
        "error_metrics_rf_df = metrics_rf_df[metrics_rf_df['Metric'].isin(['MAE', 'RMSE'])]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=error_metrics_rf_df)\n",
        "plt.title('Random Forest Regressor Model Error Metrics (Untuned)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# Plotting R-squared\n",
        "r2_metric_rf_df = metrics_rf_df[metrics_rf_df['Metric'] == 'R-squared']\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x='Metric', y='Score', data=r2_metric_rf_df)\n",
        "plt.title('Random Forest Regressor Model R-squared Score (Untuned)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1) # R-squared is typically between 0 and 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Cross- Validation & Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The Random Forest Regressor was initialized with a default number of estimators (n_estimators=100) and a random_state. No explicit hyperparameter optimization technique like Grid Search CV or Random Search CV was applied to tune the model in the provided code. The model was trained and evaluated using these default settings.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "For the Random Forest Regressor, no explicit hyperparameter optimization was performed, so there isn't a direct 'improvement' to note from tuning. The model was evaluated using its default parameters, and the performance metrics are as follows:"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3971d3a"
      },
      "source": [
        "Answer Here.\n",
        "\n",
        "### **Random Forest Regressor Model Evaluation Metrics and Business Impact**\n",
        "\n",
        "**1. Mean Absolute Error (MAE):** `8792.65`\n",
        "*   **Indication:** MAE represents the average absolute difference between the predicted salary and the actual salary. In business terms, this means that, on average, our model's salary predictions are off by approximately **$8,792.65**.\n",
        "*   **Business Impact:** This metric is highly interpretable. For HR departments, recruiters, or job seekers, an MAE of ~\\$8.8K means that if they use this model to estimate a salary, they can expect the actual salary to be within about \\$8.8K of the prediction. A lower MAE is desirable, as it indicates higher prediction accuracy, leading to more reliable salary benchmarking and negotiation strategies. This directly impacts compensation planning, budget allocation for new hires, and job offer credibility.\n",
        "\n",
        "**2. Mean Squared Error (MSE):** `167,867,995.39`\n",
        "*   **Indication:** MSE measures the average of the squares of the errors. It penalizes larger errors more heavily than MAE. The high value suggests that there might be some significant errors, or the range of salaries is large, leading to a large squared error value.\n",
        "*   **Business Impact:** While less directly interpretable than MAE in terms of raw dollar amount, MSE is crucial for understanding the presence of large prediction errors. In a business context, large MSE values mean the model can occasionally make very wrong predictions, which could lead to substantial budget miscalculations, failed negotiations due to vastly incorrect salary expectations, or even legal/ethical issues if salary estimations are far off market rates. Reducing MSE helps in ensuring more consistent and less volatile prediction accuracy.\n",
        "\n",
        "**3. Root Mean Squared Error (RMSE):** `12,956.39`\n",
        "*   **Indication:** RMSE is the square root of MSE, bringing the error back into the same units as the target variable (dollars). It's essentially a measure of the typical magnitude of the prediction errors, but, like MSE, it gives more weight to large errors.\n",
        "*   **Business Impact:** An RMSE of approximately **$12,956.39** means that the typical error magnitude is higher than the MAE, again suggesting that some predictions are further off. For businesses, RMSE provides a more sensitive measure of error. If the business prioritizes avoiding large errors (e.g., to prevent overpaying significantly or underpaying critically needed talent), then minimizing RMSE is important. It helps in assessing the overall reliability of the salary predictions and their potential deviation from actual values.\n",
        "\n",
        "**4. R-squared (R2):** `0.6192`\n",
        "*   **Indication:** R-squared, or the coefficient of determination, indicates the proportion of the variance in the dependent variable (average salary) that is predictable from the independent variables (features). An R2 of `0.6192` means that approximately **61.92%** of the variability in job salaries can be explained by our model's features.\n",
        "*   **Business Impact:** An R2 of nearly 62% suggests that the model has a reasonably good explanatory power. For business stakeholders, this implies that the selected features (job role, company size, location, industry, description content, etc.) are significant drivers of salary variation. This insight is valuable for:\n",
        "    *   **Strategic Planning:** Understanding which factors influence salaries the most helps in strategic planning for talent acquisition and retention.\n",
        "    *   **Predictive Confidence:** A higher R2 generally means greater confidence in the model's ability to predict salaries based on the given information. While 62% is good, there's still ~38% of salary variance unexplained, indicating that other factors not captured by the model (e.g., individual negotiation skills, specific skill proficiency, internal company politics, unlisted benefits) are at play. This suggests opportunities for further data collection and feature engineering to improve the model's explanatory power and thus its business utility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37b35f4e"
      },
      "source": [
        "print(\"Missing values in df_clean after dropping columns:\")\n",
        "df_clean.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation: Gradient Boosting Regressor\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor model\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Fit the Algorithm\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred_gbr = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mae_gbr = mean_absolute_error(y_test, y_pred_gbr)\n",
        "mse_gbr = mean_squared_error(y_test, y_pred_gbr)\n",
        "rmse_gbr = np.sqrt(mse_gbr)\n",
        "r2_gbr = r2_score(y_test, y_pred_gbr)\n",
        "\n",
        "print(\"Gradient Boosting Regressor Model Performance (Untuned):\")\n",
        "print(f\"  Mean Absolute Error (MAE): {mae_gbr:.2f}\")\n",
        "print(f\"  Mean Squared Error (MSE): {mse_gbr:.2f}\")\n",
        "print(f\"  Root Mean Squared Error (RMSE): {rmse_gbr:.2f}\")\n",
        "print(f\"  R-squared (R2): {r2_gbr:.4f}\")"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart for Gradient Boosting Regressor\n",
        "\n",
        "# Evaluation metrics from the Gradient Boosting Regressor model\n",
        "metrics_gbr = {'Metric': ['MAE', 'MSE', 'RMSE', 'R-squared'],\n",
        "               'Score': [mae_gbr, mse_gbr, rmse_gbr, r2_gbr]}\n",
        "metrics_gbr_df = pd.DataFrame(metrics_gbr)\n",
        "\n",
        "# Plotting Error Metrics (MAE and RMSE)\n",
        "error_metrics_gbr_df = metrics_gbr_df[metrics_gbr_df['Metric'].isin(['MAE', 'RMSE'])]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Metric', y='Score', data=error_metrics_gbr_df)\n",
        "plt.title('Gradient Boosting Regressor Model Error Metrics (Untuned)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.show()\n",
        "\n",
        "# Plotting R-squared\n",
        "r2_metric_gbr_df = metrics_gbr_df[metrics_gbr_df['Metric'] == 'R-squared']\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x='Metric', y='Score', data=r2_metric_gbr_df)\n",
        "plt.title('Gradient Boosting Regressor Model R-squared Score (Untuned)', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1) # R-squared is typically between 0 and 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?\n",
        "\n",
        " No explicit hyperparameter optimization technique like Grid Search CV or Random Search CV was used. The model was initialized with predefined parameters (n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42) and trained using these settings."
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "For the Gradient Boosting Regressor, since no explicit hyperparameter optimization was performed, there is no improvement to note from tuning. The model was evaluated using its default parameters, and its performance metrics (MAE, MSE, RMSE, R-squared) would be the same as those obtained from the initial untuned model implementation. These specific values are visible in the output of the model implementation cell."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "To determine a positive business impact, we would consider Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R2). I've outlined a plan to detail why each of these metrics is important in a business context.\n",
        "\n",
        "Explain Business Impact of Evaluation Metrics: (MAE, RMSE, R-squared) are considered for a positive business impact, and provide a justification for each. This explanation should cover how each metric translates into tangible business value and decision-making."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Based on the performance metrics, the Random Forest Regressor appears to be the best-performing model among those implemented. It has the lowest Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), and the highest R-squared (R2) score, indicating it explains more of the variance in salary than the other models. I'll update the notebook with this justification.\n",
        "\n",
        "Justify Model Choice: Generate a markdown response in cell explaining that the Random Forest Regressor was chosen as the final prediction model. The justification should compare its MAE, RMSE, and R-squared scores against the other models (Linear Regression and Gradient Boosting Regressor), highlighting its superior performance."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        " the Random Forest Regressor model and then visualize the feature importances using the model's built-built-in capabilities. Since the model was trained on data after dimensionality reduction using Truncated SVD, the feature importances will correspond to these SVD components. I will explain what this means for interpretability.\n",
        "\n",
        "Explain Random Forest Regressor: Generate a markdown response eRandom Forest Regressor model, its underlying principles, and why it is a suitable choice for this regression task.\n",
        "Calculate and Visualize Feature Importance: Extract the feature importances from the trained Random Forest Regressor model (rf_reg_model). Since the model was trained on the SVD-reduced features (X_reduced), these importances will correspond to the SVD components. Create a bar chart to visualize the top N most important SVD components.\n",
        "Explain Feature Importance Implications: Provide a markdown explanation of what the visualized feature importances indicate. Specifically address that the importances are for SVD components rather than original features, and discuss the implications of this for business interpretability."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27ae75e3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial 'Job Description' column contained 894 non-null entries and was of object data type.\n",
        "*   The text cleaning process successfully converted the text to lowercase, removed URLs, punctuation, and words/digits containing digits, storing the result in the 'cleaned\\_description' column.\n",
        "*   Common English stopwords were removed from the cleaned text, creating the 'description\\_no\\_stopwords' column.\n",
        "*   The text was tokenized into individual words, and these tokens were stored as lists in the 'description\\_tokens' column.\n",
        "*   Lemmatization was applied to reduce words to their base form, resulting in the 'description\\_lemmatized' column containing lemmatized token lists.\n",
        "*   The lemmatized tokens were joined back into strings, and TF-IDF vectorization was applied, resulting in a sparse matrix of shape (894, 5000), representing the numerical representation of the text data with the top 5000 features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}